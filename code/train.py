# Importing necessary libraries for data loading, preprocessing, model training, and database handling
import os
from load import data_load  # Function to load the dataset
from ingest_transform import preprocess  # Function for preprocessing the data
import tensorflow as tf  # TensorFlow for building and training GAN models
from tensorflow.keras import layers  # Layers for model construction
import numpy as np
import pandas as pd

# Function to define the Generator model
def build_generator(latent_dim, n_features):
    model = tf.keras.Sequential()
    model.add(layers.Dense(64, input_dim=latent_dim, activation='relu'))  # Initial dense layer with ReLU activation
    model.add(layers.Dense(128, activation='relu'))  # Hidden layer with ReLU
    model.add(layers.Dense(n_features, activation='sigmoid'))  # Output layer with sigmoid for feature generation
    return model

# Function to define the Discriminator model
def build_discriminator(n_features):
    model = tf.keras.Sequential()
    model.add(layers.Dense(128, input_dim=n_features, activation='relu'))  # Initial dense layer with ReLU activation
    model.add(layers.Dense(64, activation='relu'))  # Hidden layer with ReLU
    model.add(layers.Dense(1, activation='sigmoid'))  # Output layer with sigmoid for binary classification (real/fake)
    return model

# Function to train the GAN model, incorporating both the generator and discriminator training steps
def train_gan(generator, discriminator, gan, real_data, epochs=10000, batch_size=32, latent_dim=10):
    for epoch in range(epochs):
        # 1. Select a random batch of real data
        idx = np.random.randint(0, real_data.shape[0], batch_size)
        real_samples = real_data[idx]  # Sample real data for discriminator training

        # 2. Generate synthetic data from random noise
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        generated_samples = generator.predict(noise)  # Synthetic data generated by the generator

        # 3. Labels for real (1) and fake (0) data for discriminator training
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))

        # 4. Train the discriminator on both real and fake data
        d_loss_real = discriminator.train_on_batch(real_samples, real_labels)
        d_loss_fake = discriminator.train_on_batch(generated_samples, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)  # Average loss across real and fake samples

        # 5. Train the generator via the GAN model
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        g_loss = gan.train_on_batch(noise, real_labels)  # Use real labels to encourage generator improvement

        # Print progress every 1000 epochs for tracking model performance
        if epoch % 1000 == 0:
            print(f"Epoch: {epoch} | D Loss: {d_loss[0]} | D Accuracy: {100*d_loss[1]:.2f} | G Loss: {g_loss}")

    # Return discriminator accuracy after training completion
    return f'{100 * d_loss[1]:.2f}'

# Main function to train the GAN using the provided data
def train_(df, epochs):
    df, _ = data_load(df)  # Load and preprocess the dataset
    latent_dim = 10  # Dimension of the latent space (input noise for generator)
    n_features = df.shape[1]  # Number of features to be generated

    # Build and compile generator and discriminator models
    generator = build_generator(latent_dim, n_features)
    discriminator = build_discriminator(n_features)
    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Build and compile the GAN model (generator + discriminator)
    discriminator.trainable = False  # Freeze discriminator during generator training
    gan_input = layers.Input(shape=(latent_dim,))
    generated_sample = generator(gan_input)  # Generate synthetic sample
    gan_output = discriminator(generated_sample)  # Discriminator output for generated sample
    gan = tf.keras.Model(gan_input, gan_output)
    gan.compile(loss='binary_crossentropy', optimizer='adam')

    # Train GAN using the train_gan function
    score = train_gan(generator, discriminator, gan, df, epochs)

    # Define directory for saving trained models
    model_dir = 'data/saved_models'
    os.makedirs(model_dir, exist_ok=True)  # Create directory if it doesn't exist

    # Save the models (generator, discriminator, and GAN) in specified directory
    generator.save(os.path.join(model_dir, 'generator_model.h5'))
    discriminator.save(os.path.join(model_dir, 'discriminator_model.h5'))
    gan.save(os.path.join(model_dir, 'gan_model.h5'))

    # Return success message with model accuracy score
    return "Models trained and saved successfully!", score
